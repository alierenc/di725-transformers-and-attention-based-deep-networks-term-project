{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alierenc/di725-transformers-and-attention-based-deep-networks-term-project/blob/main/3.2.%20SigLIP-T5-Decoder%20Custom%20VLM%20-%20Image%20Captioning%20Fine-Tuning%20and%20Inference%20with%20Some%20Outputs%20Cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxRuDcZeWlmA"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = \" \" # Huggingface token\n",
        "login(token = hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSzW00Eq6ykL"
      },
      "outputs": [],
      "source": [
        "# Access google drive to save the model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import and log in wandb\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "# Initialize W&B run\n",
        "wandb.init(project=\"term-project-vision-language-model\", name=\"siglip-t5decoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieFLxaGQyKmq"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install bitsandbytes --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1KNOSns1Okr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load the dataset of full riscm\n",
        "ds = load_dataset('caglarmert/full_riscm')\n",
        "\n",
        "full = ds[\"train\"]\n",
        "\n",
        "# test   = indices [0, 3150)\n",
        "test_ds = full.select(range(3150))\n",
        "\n",
        "# validation = indices [3150, 6300)\n",
        "val_ds = full.select(range(3150, 6300))\n",
        "\n",
        "# train  = indices [6300, end)\n",
        "train_ds = full.select(range(6300, len(full)))\n",
        "\n",
        "# bundle into a DatasetDict\n",
        "ds = DatasetDict({\n",
        "    \"test\": test_ds,\n",
        "    \"val\": val_ds,\n",
        "    \"train\": train_ds,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ciL7lUpvAV",
        "outputId": "dbfaed5d-61d1-4802-a778-2364045fe9c8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224>,\n",
              " 'caption_1': 'A gray plane on the runway and the lawn beside .',\n",
              " 'caption_2': 'A grey plane is on the runway by the lawn .',\n",
              " 'caption_3': 'There is an airplane on the runway with a large lawn by the runway .',\n",
              " 'caption_4': 'A plane is parked on the runway next to the grass .',\n",
              " 'caption_5': 'There is a plane on the runway beside the grass .'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ds[\"test\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipVYrBherD78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "class CustomVLM(nn.Module):\n",
        "    def __init__(self, vision_model, language_model, vision_hidden_size, language_hidden_size):\n",
        "        super().__init__()\n",
        "        self.vision_model = vision_model\n",
        "        self.language_model = language_model\n",
        "        self.vision_proj = nn.Linear(vision_hidden_size, language_hidden_size)\n",
        "\n",
        "    def forward(self, image, input_ids=None, attention_mask=None, labels=None):\n",
        "        # Step 1: Encode image\n",
        "        vision_output = self.vision_model(pixel_values=image).last_hidden_state  # [B, N, D]\n",
        "        vision_embedding = vision_output.mean(dim=1)                             # [B, D]\n",
        "        encoder_hidden_states = self.vision_proj(vision_embedding).unsqueeze(1)  # [B, 1, d_model]\n",
        "\n",
        "        # Step 2: Package encoder output for T5\n",
        "        encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_states)\n",
        "\n",
        "        # Step 3: Decode using prompt + image context\n",
        "        output = self.language_model(\n",
        "            input_ids=input_ids,                # prompt like \"caption en\"\n",
        "            attention_mask=attention_mask,      # attention mask for prompt\n",
        "            encoder_outputs=encoder_outputs,    # SigLIP embedding as context\n",
        "            labels=labels                       # optional: ground-truth caption for training\n",
        "        )\n",
        "\n",
        "        return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdLaepmWSWD3",
        "outputId": "ac2f4d33-93b3-4e0c-86ce-20d8cb956671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    SiglipVisionModel,\n",
        "    AutoImageProcessor,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "import bitsandbytes\n",
        "import torch\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# QLoRA quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load full T5 model (needed for decoder + lm_head)\n",
        "language_model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"t5-base\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Prepare for QLoRA\n",
        "language_model = prepare_model_for_kbit_training(language_model)\n",
        "\n",
        "# Define LoRA config (on full model â€” affects both encoder & decoder if needed)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],  # extend to 'k', 'o', etc. for more aggressive tuning\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "language_model = get_peft_model(language_model, lora_config)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load SigLIP vision encoder\n",
        "vision_model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "vision_model.requires_grad_(False)\n",
        "\n",
        "# Load image processor\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "\n",
        "# Determine hidden sizes\n",
        "vision_hidden_size = vision_model.config.hidden_size     # e.g. 768\n",
        "language_hidden_size = language_model.config.d_model     # 768 for t5-base\n",
        "\n",
        "# Initialize CustomVLM with full language model\n",
        "model = CustomVLM(\n",
        "    vision_model=vision_model,\n",
        "    language_model=language_model,\n",
        "    vision_hidden_size=vision_hidden_size,\n",
        "    language_hidden_size=language_hidden_size\n",
        ")\n",
        "\n",
        "# Move to device\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwqZf20PC0JI"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnjIeiut7TRg",
        "outputId": "700319dc-cd60-4c7d-d77f-c30d6dd999a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language model device: cuda:0\n",
            "Vision model device: cuda:0\n",
            "Vision projection layer device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(\"Language model device:\", next(model.language_model.parameters()).device)\n",
        "print(\"Vision model device:\", next(model.vision_model.parameters()).device)\n",
        "print(\"Vision projection layer device:\", next(model.vision_proj.parameters()).device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nTicwe07UJl",
        "outputId": "ff8bb5c4-ed9d-4b98-bcdc-bc8b4699dd06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing the total number of parameters and the number of trainable parameters:\n",
            "Vision Encoder (SigLIP): {'Total': 92884224, 'Trainable': 0}\n",
            "Vision Projection Layer: {'Total': 590592, 'Trainable': 590592}\n",
            "Language Model (full T5): {'Total': 153009408, 'Trainable': 884736}\n",
            "T5 Decoder (only): {'Total': 96071808, 'Trainable': 589824}\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(module):\n",
        "    total = sum(p.numel() for p in module.parameters())\n",
        "    trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    return {\"Total\": total, \"Trainable\": trainable}\n",
        "\n",
        "print(\"Printing the total number of parameters and the number of trainable parameters:\")\n",
        "print(\"Vision Encoder (SigLIP):\", count_parameters(model.vision_model))\n",
        "print(\"Vision Projection Layer:\", count_parameters(model.vision_proj))\n",
        "print(\"Language Model (full T5):\", count_parameters(model.language_model))\n",
        "\n",
        "# Count only T5 decoder (within the full model)\n",
        "print(\"T5 Decoder (only):\", count_parameters(model.language_model.base_model.decoder))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9lTIhyzqsiz",
        "outputId": "bcb68b5e-249b-495b-a718-3717b31b28fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating caption for sample 1\n",
            "Generated caption: 'jurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjurjur'\n",
            "\n",
            "Generating caption for sample 2\n",
            "Generated caption: ''\n",
            "\n",
            "Generating caption for sample 3\n",
            "Generated caption: 'dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s dÃ©s'\n",
            "\n",
            "Generating caption for sample 4\n",
            "Generated caption: 'virvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvir'\n",
            "\n",
            "Generating caption for sample 5\n",
            "Generated caption: 'drondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondrondron'\n",
            "\n",
            "Generating caption for sample 6\n",
            "Generated caption: 'Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi Zi'\n",
            "\n",
            "Generating caption for sample 7\n",
            "Generated caption: ''\n",
            "\n",
            "Generating caption for sample 8\n",
            "Generated caption: 'rumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrumrum'\n",
            "\n",
            "Generating caption for sample 9\n",
            "Generated caption: 'raisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisraisrais'\n",
            "\n",
            "Generating caption for sample 10\n",
            "Generated caption: 'virvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvirvir'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "# We check whether the model can produce captions\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "max_new_tokens = 30\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Generating caption for sample {i + 1}\")\n",
        "\n",
        "    # Preprocess image\n",
        "    image = ds[\"test\"][i][\"image\"]\n",
        "    pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vision encoding inside VLM forward\n",
        "        vision_output = model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
        "        vision_embedding = vision_output.mean(dim=1)\n",
        "        encoder_hidden_states = model.vision_proj(vision_embedding).unsqueeze(1)\n",
        "\n",
        "        # Wrap as BaseModelOutput\n",
        "        encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_states)\n",
        "\n",
        "        # Prompt setup\n",
        "        prompt = \"caption en\"\n",
        "        tokenized = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        input_ids = tokenized[\"input_ids\"]\n",
        "        attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "        # Remove <pad> if it appears as the first token\n",
        "        if input_ids[0, 0] == tokenizer.pad_token_id:\n",
        "            input_ids = input_ids[:, 1:]\n",
        "            attention_mask = attention_mask[:, 1:]\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.language_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # Decode output\n",
        "        decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        caption = decoded.replace(prompt, \"\").strip()\n",
        "        caption = decoded.replace(\"<pad>\", \"\").strip()\n",
        "        print(\"Generated caption:\", repr(caption))\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89tTN1e8xTjl",
        "outputId": "9226c23d-1e67-4ee9-9ed0-0e2f2b3398e1"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/10:   0%|          | 0/150 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Epoch 1/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:39<00:00,  1.86s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed. Average Train Loss: 5.7107\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating:   0%|          | 0/13 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.34s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Validation Loss: 4.3752\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:40<00:00,  1.87s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed. Average Train Loss: 4.2606\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.36s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Validation Loss: 2.8540\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:40<00:00,  1.87s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 completed. Average Train Loss: 3.0588\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.33s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Validation Loss: 2.0175\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:40<00:00,  1.87s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 completed. Average Train Loss: 2.5203\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.34s/it]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Validation Loss: 1.7390\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:41<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 completed. Average Train Loss: 2.2820\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.5744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:40<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed. Average Train Loss: 2.1308\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.4601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:41<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 completed. Average Train Loss: 2.0189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.3625\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:40<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed. Average Train Loss: 1.9280\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.2900\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:41<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 completed. Average Train Loss: 1.8566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.34s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.2275\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 150/150 [04:40<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 completed. Average Train Loss: 1.7949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13/13 [00:17<00:00,  1.34s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.1789\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Collate function for T5 captioning\n",
        "def collate_fn(batch):\n",
        "    images = [image_processor(example[\"image\"], return_tensors=\"pt\")[\"pixel_values\"].squeeze(0) for example in batch]\n",
        "    captions = [example[\"caption_3\"] for example in batch]\n",
        "\n",
        "    pixel_values = torch.stack(images)\n",
        "\n",
        "    # For T5: prompt goes into input_ids, caption goes into labels\n",
        "    prompts = [\"caption en\"] * len(captions)\n",
        "    tokenized_input = tokenizer(prompts, padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    tokenized_labels = tokenizer(captions, padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": tokenized_input[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_input[\"attention_mask\"],\n",
        "        \"labels\": tokenized_labels[\"input_ids\"]  # T5 will shift internally\n",
        "    }\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(ds[\"train\"], batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ds[\"val\"], batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Replace <pad> tokens in labels with -100 so they are ignored in loss\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            image=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "        wandb.log({\n",
        "            \"train/loss\": loss.item(),\n",
        "            \"train/step\": epoch * len(train_loader) + step\n",
        "        })\n",
        "\n",
        "\n",
        "    avg_train_loss = total_loss / total_samples\n",
        "    print(f\"Epoch {epoch+1} completed. Average Train Loss: {avg_train_loss:.4f}\")\n",
        "    wandb.log({\"train/avg_epoch_loss\": avg_train_loss, \"epoch\": epoch + 1})\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Same label cleaning\n",
        "            labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "            outputs = model(\n",
        "                image=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            val_loss += outputs.loss.item() * batch_size\n",
        "            val_samples += batch_size\n",
        "\n",
        "    avg_val_loss = val_loss / val_samples\n",
        "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "    wandb.log({\"val/loss\": avg_val_loss, \"epoch\": epoch + 1})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFK9pKxT-Jhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa568bf9-131d-4a42-c9df-5b6d0d158584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating captions: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3150/3150 [54:16<00:00,  1.03s/it]\n"
          ]
        }
      ],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "max_new_tokens = 30\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "predictions = []\n",
        "\n",
        "for i in tqdm(range(len(ds[\"test\"])), desc=\"Generating captions\"):\n",
        "    # Preprocess image\n",
        "    image = ds[\"test\"][i][\"image\"]\n",
        "    pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vision encoding inside VLM forward\n",
        "        vision_output = model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
        "        vision_embedding = vision_output.mean(dim=1)\n",
        "        encoder_hidden_states = model.vision_proj(vision_embedding).unsqueeze(1)\n",
        "\n",
        "        # Wrap as BaseModelOutput\n",
        "        encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_states)\n",
        "\n",
        "        # Prompt setup\n",
        "        prompt = \"caption en\"\n",
        "        tokenized = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        input_ids = tokenized[\"input_ids\"]\n",
        "        attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "        # Remove <pad> if it appears as the first token\n",
        "        if input_ids[0, 0] == tokenizer.pad_token_id:\n",
        "            input_ids = input_ids[:, 1:]\n",
        "            attention_mask = attention_mask[:, 1:]\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.language_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # Decode output\n",
        "        decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        caption = decoded.replace(prompt, \"\").strip()\n",
        "        caption = decoded.replace(\"<pad>\", \"\").strip()\n",
        "        predictions.append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ial5sVoWBNqT"
      },
      "outputs": [],
      "source": [
        "# Get the references\n",
        "# Define a varible to store the reference captions\n",
        "all_references = []\n",
        "for i in range(len(ds[\"test\"])):\n",
        "    # Get the reference\n",
        "    reference_per_sample = []\n",
        "    for j in range(1,6):\n",
        "        reference = ds[\"test\"][i][f\"caption_{j}\"]\n",
        "        reference_per_sample.append(reference)\n",
        "        print(f\"The reference caption_{j}:\")\n",
        "        print(repr(reference))\n",
        "\n",
        "    print()\n",
        "    all_references.append(reference_per_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtqWQf7DEKl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312b893f-ac7c-406b-c1dc-14b98b8c136a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['A gray plane on the runway and the lawn beside .', 'A grey plane is on the runway by the lawn .', 'There is an airplane on the runway with a large lawn by the runway .', 'A plane is parked on the runway next to the grass .', 'There is a plane on the runway beside the grass .'], ['Three small planes parked in a line on the airport and a big plane behind them .', 'There are four aircraft on the open ground, The largest of which is three times as large as the smallest one .', 'There are many planes of different sizes in a clearing .', 'Four planes are parked on the runway .', 'Four planes of different sizes were on the marked ground .'], ['A plane parked in a line on the airport with some marks .', 'A white plane was parked on the instruction line .', 'An airplane parked in an open area with many containers next to it .', 'A plane is parked on the open space .', 'There is 1 plane on the ground marked .'], ['A small plane and a big plane parked next to boarding bridges .', 'A white plane and a gray plane parked at the boarding port  The white plane was about four times as large as the gray one .', 'Two planes of different sizes are neatly parked next to the buildings in the airport .', 'A large plane and a small plane are parked near the terminal .', 'Two planes are on the marked ground .'], ['Two planes parked next to boarding bridges .', 'Two aircraft were parked at the departure gates .', 'Two planes of different sizes are neatly parked next to the buildings in the airport .', 'Two planes are parked next to the terminal .', 'Two planes are on the marked ground .']]\n"
          ]
        }
      ],
      "source": [
        "# Check the format of the reference captions\n",
        "print(all_references[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hS827PUEhRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fc7f7fb-c0ca-45c4-a1c1-484ce2179c33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There are several runways on the leased land next to the road . There are several cars on the freeway . There are several cars', 'There were two airplanes in the clearing at the airport . Two airplanes were cleared by the airport . There were three airplanes parked', 'There are several airplanes in the airport . Two airplanes parked in the airport . There are many buildings . There are many buildings', 'An airplane is parked in the airport . There are many buildings parked in the airport . There are many planes parked in the', 'There were two planes of different sizes of different sizes in the airport . Two different planes . Two different sizes of different sizes .']\n"
          ]
        }
      ],
      "source": [
        "# Check the format of the predicted captions. Each sample starts with a new line\n",
        "print(predictions[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6-ypijV2XMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b61e7c43-6a56-4443-d9d6-9e3cc354cc2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize references and predictions:\n",
        "tokenized_refs = [\n",
        "    [nltk.word_tokenize(ref.lower()) for ref in refs]\n",
        "    for refs in all_references\n",
        "]\n",
        "\n",
        "tokenized_hyps = [nltk.word_tokenize(pred.lower()) for pred in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HVsAM-UjEtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b45a8d74-dbbb-46cc-ccf7-48ac0875339a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['a',\n",
              "  'gray',\n",
              "  'plane',\n",
              "  'on',\n",
              "  'the',\n",
              "  'runway',\n",
              "  'and',\n",
              "  'the',\n",
              "  'lawn',\n",
              "  'beside',\n",
              "  '.'],\n",
              " ['a', 'grey', 'plane', 'is', 'on', 'the', 'runway', 'by', 'the', 'lawn', '.'],\n",
              " ['there',\n",
              "  'is',\n",
              "  'an',\n",
              "  'airplane',\n",
              "  'on',\n",
              "  'the',\n",
              "  'runway',\n",
              "  'with',\n",
              "  'a',\n",
              "  'large',\n",
              "  'lawn',\n",
              "  'by',\n",
              "  'the',\n",
              "  'runway',\n",
              "  '.'],\n",
              " ['a',\n",
              "  'plane',\n",
              "  'is',\n",
              "  'parked',\n",
              "  'on',\n",
              "  'the',\n",
              "  'runway',\n",
              "  'next',\n",
              "  'to',\n",
              "  'the',\n",
              "  'grass',\n",
              "  '.'],\n",
              " ['there',\n",
              "  'is',\n",
              "  'a',\n",
              "  'plane',\n",
              "  'on',\n",
              "  'the',\n",
              "  'runway',\n",
              "  'beside',\n",
              "  'the',\n",
              "  'grass',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tokenized_refs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9BGh1zclFI-"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-2\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/2, 1/2),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-2: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LFVQTlOIWJs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9252d65a-6edc-49de-be2c-18d9be9d069f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus BLEU-2: 37.53\n"
          ]
        }
      ],
      "source": [
        "# Corpus-level BLEU-2\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/2, 1/2),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-2: {corpus_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOQ1uwXglOJF"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-3\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/3, 1/3, 1/3),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-3: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DRXEFMkIYqu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1c6e8d9-6a3f-45db-eea3-42e6c90f1c66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus BLEU-3: 29.04\n"
          ]
        }
      ],
      "source": [
        "# Corpus-level BLEU-3\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/3, 1/3, 1/3),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-3: {corpus_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52GwUtcJk_ki"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-4\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/4, 1/4, 1/4, 1/4),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-4: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mM7yA4qEIbG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a778ddb5-6baa-479c-d212-801e6ed149f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus BLEU-4: 22.73\n"
          ]
        }
      ],
      "source": [
        "# Corpus-level BLEU-4\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/4, 1/4, 1/4, 1/4),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-4: {corpus_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFe4Ho28BBA-"
      },
      "outputs": [],
      "source": [
        "# Go on to calculate ROUGE scores\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndmk9F4VaR9z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure tokenizer\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def rouge_n(ref: str, hyp: str, n: int = 4):\n",
        "    ref_toks = nltk.word_tokenize(ref.lower())\n",
        "    hyp_toks = nltk.word_tokenize(hyp.lower())\n",
        "    ref_ngrams = list(nltk.ngrams(ref_toks, n))\n",
        "    hyp_ngrams = list(nltk.ngrams(hyp_toks, n))\n",
        "    ref_counts = Counter(ref_ngrams)\n",
        "    hyp_counts = Counter(hyp_ngrams)\n",
        "    overlap = sum(min(ref_counts[ng], hyp_counts[ng]) for ng in ref_counts)\n",
        "    recall = overlap / max(len(ref_ngrams), 1)\n",
        "    precision = overlap / max(len(hyp_ngrams), 1)\n",
        "    f1 = 2 * recall * precision / (recall + precision + 1e-8)\n",
        "    return (recall, precision, f1)\n",
        "\n",
        "# Compute ROUGE-2\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=2)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-2 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-2 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-2 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha0vcqTFYx79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf25f32-df7e-451e-8f04-e9b34dc5ea6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AVERAGE ROUGE-2 METRICS ===\n",
            "Recall:    47.23\n",
            "Precision: 22.40\n",
            "F1:        29.56\n"
          ]
        }
      ],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-2 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3iJDsYWmLq3"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE-3\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=3)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-3 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-3 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-3 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz9cBH2KY1mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ed1d5b6-8f56-4333-9a05-a64d6282dda0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AVERAGE ROUGE-3 METRICS ===\n",
            "Recall:    34.11\n",
            "Precision: 14.85\n",
            "F1:        20.10\n"
          ]
        }
      ],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-3 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GbFHU6QnYuJ"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE-4\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=4)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-4 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-4 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-4 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MlAOzLHs9je",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0c3c8e8-af40-4cd2-8b1c-aca5dc4d8490"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AVERAGE ROUGE-4 METRICS ===\n",
            "Recall:    24.80\n",
            "Precision: 10.08\n",
            "F1:        13.90\n"
          ]
        }
      ],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-4 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_Dq8sYbJLon"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/DI725 - Transformers and Attention-based Deep Networks/Term Project/siglip-t5-custom_vlm_finetuned\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Save merged language model (now a clean T5)\n",
        "model.language_model.save_pretrained(save_dir)\n",
        "\n",
        "# Save vision encoder and image processor\n",
        "model.vision_model.save_pretrained(f\"{save_dir}/vision_encoder\")\n",
        "image_processor.save_pretrained(f\"{save_dir}/vision_encoder\")\n",
        "\n",
        "# Save vision projection layer\n",
        "torch.save(model.vision_proj.state_dict(), f\"{save_dir}/vision_proj.pt\")\n",
        "\n",
        "# Save config for reinitialization\n",
        "import json\n",
        "config = {\n",
        "    \"vision_encoder_path\": \"vision_encoder\",\n",
        "    \"language_model_path\": \".\",\n",
        "    \"vision_proj_path\": \"vision_proj.pt\",\n",
        "    \"vision_hidden_size\": model.vision_proj.in_features,\n",
        "    \"language_hidden_size\": model.vision_proj.out_features\n",
        "}\n",
        "with open(os.path.join(save_dir, \"custom_vlm_config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4hhPU2Z93U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d40d7cc-5c29-4650-d9cf-be8fc90c6f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE\n"
          ]
        }
      ],
      "source": [
        "print(\"DONE\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMbkbIv1ZWEAvFH8SqBCgzf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}