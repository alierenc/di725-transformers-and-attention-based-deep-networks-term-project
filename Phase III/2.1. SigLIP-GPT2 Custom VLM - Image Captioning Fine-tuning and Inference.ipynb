{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alierenc/di725-transformers-and-attention-based-deep-networks-term-project/blob/main/Phase%20III/2.1.%20SigLIP-GPT2%20Custom%20VLM%20-%20Image%20Captioning%20Fine-tuning%20and%20Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxRuDcZeWlmA"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = \" \" # Huggingface token\n",
        "login(token = hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSzW00Eq6ykL"
      },
      "outputs": [],
      "source": [
        "# Access google drive to save the model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import and log in wandb\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "# Initialize W&B run\n",
        "wandb.init(project=\"term-project-vision-language-model\", name=\"siglip-gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieFLxaGQyKmq"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install bitsandbytes --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1KNOSns1Okr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load the dataset of full riscm\n",
        "ds = load_dataset('caglarmert/full_riscm')\n",
        "\n",
        "full = ds[\"train\"]\n",
        "\n",
        "# test   = indices [0, 3150)\n",
        "test_ds = full.select(range(3150))\n",
        "\n",
        "# validation = indices [3150, 6300)\n",
        "val_ds = full.select(range(3150, 6300))\n",
        "\n",
        "# train  = indices [6300, end)\n",
        "train_ds = full.select(range(6300, len(full)))\n",
        "\n",
        "# bundle into a DatasetDict\n",
        "ds = DatasetDict({\n",
        "    \"test\": test_ds,\n",
        "    \"val\": val_ds,\n",
        "    \"train\": train_ds,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ciL7lUpvAV"
      },
      "outputs": [],
      "source": [
        "ds[\"test\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdLaepmWSWD3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomVLM(nn.Module):\n",
        "    def __init__(self, vision_model, language_model, vision_hidden_size, language_hidden_size):\n",
        "        super(CustomVLM, self).__init__()\n",
        "        self.vision_model = vision_model\n",
        "        self.language_model = language_model\n",
        "        self.vision_proj = nn.Linear(vision_hidden_size, language_hidden_size)\n",
        "\n",
        "    def forward(self, image, input_ids=None, attention_mask=None, labels=None):\n",
        "        # Encode image\n",
        "        vision_output = self.vision_model(pixel_values=image).last_hidden_state\n",
        "        vision_embedding = torch.mean(vision_output, dim=1)\n",
        "        projected_embedding = self.vision_proj(vision_embedding)  # [B, D]\n",
        "        prefix = projected_embedding.unsqueeze(1)  # [B, 1, D]\n",
        "\n",
        "        # Get text embeddings\n",
        "        inputs_embeds = self.language_model.transformer.wte(input_ids)  # [B, T, D]\n",
        "        inputs_embeds = torch.cat([prefix, inputs_embeds], dim=1)\n",
        "\n",
        "        # Create or update attention mask\n",
        "        batch_size = inputs_embeds.size(0)\n",
        "        seq_len = inputs_embeds.size(1)\n",
        "\n",
        "        if attention_mask is None:\n",
        "            # No mask was passed -> assume no padding in input\n",
        "            attention_mask = torch.ones((batch_size, input_ids.size(1)), dtype=torch.long, device=input_ids.device)\n",
        "\n",
        "        # Prefix mask for vision token\n",
        "        prefix_mask = torch.ones((batch_size, 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "        attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "        return self.language_model(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJecHz013Am8"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    SiglipVisionModel,\n",
        "    AutoImageProcessor,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "import bitsandbytes\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load quantized GPT-2\n",
        "language_model = GPT2LMHeadModel.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Prepare for QLoRA\n",
        "language_model = prepare_model_for_kbit_training(language_model)\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "language_model = get_peft_model(language_model, lora_config)\n",
        "language_hidden_size = language_model.config.n_embd  # Usually 768\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load SigLIP\n",
        "vision_model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "vision_model.requires_grad_(False)\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "vision_hidden_size = vision_model.config.hidden_size\n",
        "\n",
        "# Final model\n",
        "model = CustomVLM(\n",
        "    vision_model=vision_model,\n",
        "    language_model=language_model,\n",
        "    vision_hidden_size=vision_hidden_size,\n",
        "    language_hidden_size=language_hidden_size\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmyL4wRymzjg"
      },
      "outputs": [],
      "source": [
        "print(\"Language model device:\", next(model.language_model.parameters()).device)\n",
        "print(\"Vision model device:\", next(model.vision_model.parameters()).device)\n",
        "print(\"Vision projection layer device:\", next(model.vision_proj.parameters()).device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-5LmbTaJtny"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIGXaH1XjdzM"
      },
      "outputs": [],
      "source": [
        "# We count the number of parameters\n",
        "def count_parameters(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return {\"Total\": total, \"Trainable\": trainable}\n",
        "\n",
        "print(\"Printing the total number of parameters and the number of trainable parameters:\")\n",
        "print(\"Vision Encoder (SigLIP):\", count_parameters(model.vision_model))\n",
        "print(\"Language Decoder (GPT-2):\", count_parameters(model.language_model))\n",
        "print(\"Vision Projection Layer:\", count_parameters(model.vision_proj))\n",
        "print(\"Total CustomVLM:\", count_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgM9u7Rmjho_"
      },
      "outputs": [],
      "source": [
        "# Check whether the model can generate captions at all\n",
        "# Set model to eval mode\n",
        "model.eval()\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Generating caption for sample {i + 1}\")\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = ds[\"test\"][i][\"image\"]\n",
        "    pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "    pixel_values = pixel_values.to(model.language_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vision encoding\n",
        "        vision_output = model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
        "        vision_embedding = vision_output.mean(dim=1)\n",
        "        vision_proj = model.vision_proj(vision_embedding).unsqueeze(1)  # [B, 1, D]\n",
        "\n",
        "        # Prepare prompt\n",
        "        prompt = \"caption en\"\n",
        "        tokenized = tokenizer(prompt, return_tensors=\"pt\", padding=False)\n",
        "        input_ids = tokenized[\"input_ids\"].to(model.language_model.device)\n",
        "        attention_mask = tokenized[\"attention_mask\"].to(model.language_model.device)\n",
        "\n",
        "        # Text embedding\n",
        "        input_embeds = model.language_model.transformer.wte(input_ids)\n",
        "        input_embeds = torch.cat([vision_proj, input_embeds], dim=1)\n",
        "\n",
        "        # Extend attention mask with prefix mask\n",
        "        prefix_mask = torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype).to(attention_mask.device)\n",
        "        attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "        # Generate text\n",
        "        generated_ids = model.language_model.generate(\n",
        "            inputs_embeds=input_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=30,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # Decode the caption\n",
        "        caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)[len(prompt):]\n",
        "        print(\"Generated caption:\", repr(caption))\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89tTN1e8xTjl"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prefix for all captions\n",
        "caption_prefix = \"caption en\"\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [image_processor(example[\"image\"], return_tensors=\"pt\")[\"pixel_values\"].squeeze(0) for example in batch]\n",
        "    captions = [caption_prefix + example[\"caption_3\"] for example in batch]\n",
        "    pixel_values = torch.stack(images)\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        captions,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"labels\": tokenized[\"input_ids\"].clone()\n",
        "    }\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(ds[\"train\"], batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ds[\"val\"], batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Pad labels for prefix position with -100\n",
        "        prefix_ignore = torch.full((labels.size(0), 1), -100, dtype=labels.dtype, device=labels.device)\n",
        "        labels = torch.cat([prefix_ignore, labels], dim=1)\n",
        "\n",
        "        # Add prefix to attention mask\n",
        "        prefix_mask = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "        extended_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "\n",
        "        outputs = model(\n",
        "            image=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "        wandb.log({\n",
        "            \"train/loss\": loss.item(),\n",
        "            \"train/step\": epoch * len(train_loader) + step\n",
        "        })\n",
        "\n",
        "    avg_train_loss = total_loss / total_samples\n",
        "    print(f\"Epoch {epoch+1} completed. Avg Train Loss: {avg_train_loss:.4f}\")\n",
        "    wandb.log({\"train/avg_epoch_loss\": avg_train_loss, \"epoch\": epoch + 1})\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Pad labels for prefix position with -100\n",
        "            prefix_ignore = torch.full((labels.size(0), 1), -100, dtype=labels.dtype, device=labels.device)\n",
        "            labels = torch.cat([prefix_ignore, labels], dim=1)\n",
        "\n",
        "            prefix_mask = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "            extended_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "            outputs = model(\n",
        "                image=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=extended_attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            val_loss += outputs.loss.item() * batch_size\n",
        "            val_samples += batch_size\n",
        "\n",
        "    avg_val_loss = val_loss / val_samples\n",
        "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
        "    wandb.log({\"val/loss\": avg_val_loss, \"epoch\": epoch + 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFK9pKxT-Jhv"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "# Loop through the full test dataset\n",
        "for i in tqdm(range(len(ds[\"test\"])), desc=\"Generating captions\"):\n",
        "    image = ds[\"test\"][i][\"image\"]\n",
        "    pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(model.language_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vision encoding\n",
        "        vision_output = model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
        "        vision_embedding = vision_output.mean(dim=1)\n",
        "        vision_proj = model.vision_proj(vision_embedding).unsqueeze(1)  # [B, 1, D]\n",
        "\n",
        "        # Prepare prompt\n",
        "        prompt = \"caption en\"\n",
        "        tokenized = tokenizer(prompt, return_tensors=\"pt\", padding=False)\n",
        "        input_ids = tokenized[\"input_ids\"].to(model.language_model.device)\n",
        "        attention_mask = tokenized[\"attention_mask\"].to(model.language_model.device)\n",
        "\n",
        "        # Text embedding\n",
        "        input_embeds = model.language_model.transformer.wte(input_ids)\n",
        "        input_embeds = torch.cat([vision_proj, input_embeds], dim=1)\n",
        "\n",
        "        # Extend attention mask with prefix mask\n",
        "        prefix_mask = torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype).to(attention_mask.device)\n",
        "        attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "        # Generate text\n",
        "        generated_ids = model.language_model.generate(\n",
        "            inputs_embeds=input_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=30,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # Decode caption (removing prompt part)\n",
        "        caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)[len(prompt):].strip()\n",
        "        predictions.append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ial5sVoWBNqT"
      },
      "outputs": [],
      "source": [
        "# Get the references\n",
        "# Define a variable to store the reference captions\n",
        "all_references = []\n",
        "for i in tqdm(range(len(ds[\"test\"])), desc=\"Collecting reference captions\"):\n",
        "    # Get the reference\n",
        "    reference_per_sample = []\n",
        "    for j in range(1,6):\n",
        "        reference = ds[\"test\"][i][f\"caption_{j}\"]\n",
        "        reference_per_sample.append(reference)\n",
        "        print(f\"The reference caption_{j}:\")\n",
        "        print(repr(reference))\n",
        "\n",
        "    print()\n",
        "    all_references.append(reference_per_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtqWQf7DEKl4"
      },
      "outputs": [],
      "source": [
        "# Check the format of the reference captions\n",
        "print(all_references[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hS827PUEhRN"
      },
      "outputs": [],
      "source": [
        "# Check the format of the predicted captions. Each sample starts with a new line\n",
        "print(predictions[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6-ypijV2XMP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize references and predictions:\n",
        "tokenized_refs = [\n",
        "    [nltk.word_tokenize(ref.lower()) for ref in refs]\n",
        "    for refs in all_references\n",
        "]\n",
        "\n",
        "tokenized_hyps = [nltk.word_tokenize(pred.lower()) for pred in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HVsAM-UjEtN"
      },
      "outputs": [],
      "source": [
        "tokenized_refs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9BGh1zclFI-"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-2\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/2, 1/2),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-2: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puRb6hgwEBAs"
      },
      "outputs": [],
      "source": [
        "# Corpus-level BLEU-2\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/2, 1/2),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-2: {corpus_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOQ1uwXglOJF"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-3\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/3, 1/3, 1/3),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-3: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S23MU9ALECXX"
      },
      "outputs": [],
      "source": [
        "# Corpus-level BLEU-3\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/3, 1/3, 1/3),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-3: {corpus_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52GwUtcJk_ki"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-4\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/4, 1/4, 1/4, 1/4),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-4: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus-level BLEU-4\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/4, 1/4, 1/4, 1/4),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-4: {corpus_score*100:.2f}\")"
      ],
      "metadata": {
        "id": "b4KU-DsACcux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFe4Ho28BBA-"
      },
      "outputs": [],
      "source": [
        "# Go on to calculate ROUGE scores\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndmk9F4VaR9z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure tokenizer\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def rouge_n(ref: str, hyp: str, n: int = 4):\n",
        "    ref_toks = nltk.word_tokenize(ref.lower())\n",
        "    hyp_toks = nltk.word_tokenize(hyp.lower())\n",
        "    ref_ngrams = list(nltk.ngrams(ref_toks, n))\n",
        "    hyp_ngrams = list(nltk.ngrams(hyp_toks, n))\n",
        "    ref_counts = Counter(ref_ngrams)\n",
        "    hyp_counts = Counter(hyp_ngrams)\n",
        "    overlap = sum(min(ref_counts[ng], hyp_counts[ng]) for ng in ref_counts)\n",
        "    recall = overlap / max(len(ref_ngrams), 1)\n",
        "    precision = overlap / max(len(hyp_ngrams), 1)\n",
        "    f1 = 2 * recall * precision / (recall + precision + 1e-8)\n",
        "    return (recall, precision, f1)\n",
        "\n",
        "# Compute ROUGE-2\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=2)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-2 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-2 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-2 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha0vcqTFYx79"
      },
      "outputs": [],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-2 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3iJDsYWmLq3"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE-3\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=3)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-3 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-3 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-3 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz9cBH2KY1mv"
      },
      "outputs": [],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-3 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GbFHU6QnYuJ"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE-4\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=4)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-4 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-4 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-4 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MlAOzLHs9je"
      },
      "outputs": [],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-4 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4hhPU2Z93U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/DI725 - Transformers and Attention-based Deep Networks/Term Project/siglip-gpt2-custom_vlm_finetuned\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Save merged language model (now a clean GPT-2)\n",
        "model.language_model.save_pretrained(save_dir)\n",
        "\n",
        "# Save vision encoder and image processor\n",
        "model.vision_model.save_pretrained(f\"{save_dir}/vision_encoder\")\n",
        "image_processor.save_pretrained(f\"{save_dir}/vision_encoder\")\n",
        "\n",
        "# Save vision projection layer\n",
        "torch.save(model.vision_proj.state_dict(), f\"{save_dir}/vision_proj.pt\")\n",
        "\n",
        "# Save config for reinitialization\n",
        "import json\n",
        "config = {\n",
        "    \"vision_encoder_path\": \"vision_encoder\",\n",
        "    \"language_model_path\": \".\",\n",
        "    \"vision_proj_path\": \"vision_proj.pt\",\n",
        "    \"vision_hidden_size\": model.vision_proj.in_features,\n",
        "    \"language_hidden_size\": model.vision_proj.out_features\n",
        "}\n",
        "with open(os.path.join(save_dir, \"custom_vlm_config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DONE\")"
      ],
      "metadata": {
        "id": "eBey5YrPGnY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AvaU2cHRC4jE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyMz8dyeLzXOknEf2umaW+tD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}