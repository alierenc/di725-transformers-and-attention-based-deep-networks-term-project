{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alierenc/di725-transformers-and-attention-based-deep-networks-term-project/blob/main/Phase%20III/2.2.%20SigLIP-GPT2%20Custom%20VLM%20-%20Image%20Captioning%20Fine-tuning%20and%20Inference%20with%20some%20Outputs%20Cleaned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxRuDcZeWlmA"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = \" \" # Huggingface token\n",
        "login(token = hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSzW00Eq6ykL"
      },
      "outputs": [],
      "source": [
        "# Access google drive to save the model\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Import and log in wandb\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "# Initialize W&B run\n",
        "wandb.init(project=\"term-project-vision-language-model\", name=\"siglip-gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieFLxaGQyKmq"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install bitsandbytes --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1KNOSns1Okr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load the dataset of full riscm\n",
        "ds = load_dataset('caglarmert/full_riscm')\n",
        "\n",
        "full = ds[\"train\"]\n",
        "\n",
        "# test   = indices [0, 3150)\n",
        "test_ds = full.select(range(3150))\n",
        "\n",
        "# validation = indices [3150, 6300)\n",
        "val_ds = full.select(range(3150, 6300))\n",
        "\n",
        "# train  = indices [6300, end)\n",
        "train_ds = full.select(range(6300, len(full)))\n",
        "\n",
        "# bundle into a DatasetDict\n",
        "ds = DatasetDict({\n",
        "    \"test\": test_ds,\n",
        "    \"val\": val_ds,\n",
        "    \"train\": train_ds,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ciL7lUpvAV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03732086-955b-49a4-cd11-d2a3bbb2feaf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=224x224>,\n",
              " 'caption_1': 'A gray plane on the runway and the lawn beside .',\n",
              " 'caption_2': 'A grey plane is on the runway by the lawn .',\n",
              " 'caption_3': 'There is an airplane on the runway with a large lawn by the runway .',\n",
              " 'caption_4': 'A plane is parked on the runway next to the grass .',\n",
              " 'caption_5': 'There is a plane on the runway beside the grass .'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "ds[\"test\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdLaepmWSWD3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomVLM(nn.Module):\n",
        "    def __init__(self, vision_model, language_model, vision_hidden_size, language_hidden_size):\n",
        "        super(CustomVLM, self).__init__()\n",
        "        self.vision_model = vision_model\n",
        "        self.language_model = language_model\n",
        "        self.vision_proj = nn.Linear(vision_hidden_size, language_hidden_size)\n",
        "\n",
        "    def forward(self, image, input_ids=None, attention_mask=None, labels=None):\n",
        "        # Encode image\n",
        "        vision_output = self.vision_model(pixel_values=image).last_hidden_state\n",
        "        vision_embedding = torch.mean(vision_output, dim=1)\n",
        "        projected_embedding = self.vision_proj(vision_embedding)  # [B, D]\n",
        "        prefix = projected_embedding.unsqueeze(1)  # [B, 1, D]\n",
        "\n",
        "        # Get text embeddings\n",
        "        inputs_embeds = self.language_model.transformer.wte(input_ids)  # [B, T, D]\n",
        "        inputs_embeds = torch.cat([prefix, inputs_embeds], dim=1)\n",
        "\n",
        "        # Create or update attention mask\n",
        "        batch_size = inputs_embeds.size(0)\n",
        "        seq_len = inputs_embeds.size(1)\n",
        "\n",
        "        if attention_mask is None:\n",
        "            # No mask was passed -> assume no padding in input\n",
        "            attention_mask = torch.ones((batch_size, input_ids.size(1)), dtype=torch.long, device=input_ids.device)\n",
        "\n",
        "        # Prefix mask for vision token\n",
        "        prefix_mask = torch.ones((batch_size, 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "        attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "        return self.language_model(\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJecHz013Am8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2176f9-87d0-46f7-d6bf-2458f2846c7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        }
      ],
      "source": [
        "from transformers import (\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2Tokenizer,\n",
        "    SiglipVisionModel,\n",
        "    AutoImageProcessor,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "import bitsandbytes\n",
        "\n",
        "# Quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load quantized GPT-2\n",
        "language_model = GPT2LMHeadModel.from_pretrained(\n",
        "    \"gpt2\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Prepare for QLoRA\n",
        "language_model = prepare_model_for_kbit_training(language_model)\n",
        "\n",
        "# LoRA config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"c_attn\", \"c_proj\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "language_model = get_peft_model(language_model, lora_config)\n",
        "language_hidden_size = language_model.config.n_embd  # Usually 768\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load SigLIP\n",
        "vision_model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "vision_model.requires_grad_(False)\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "vision_hidden_size = vision_model.config.hidden_size\n",
        "\n",
        "# Final model\n",
        "model = CustomVLM(\n",
        "    vision_model=vision_model,\n",
        "    language_model=language_model,\n",
        "    vision_hidden_size=vision_hidden_size,\n",
        "    language_hidden_size=language_hidden_size\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmyL4wRymzjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7708cfe-228c-4755-c4cb-39a7cb4a75c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language model device: cuda:0\n",
            "Vision model device: cuda:0\n",
            "Vision projection layer device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "print(\"Language model device:\", next(model.language_model.parameters()).device)\n",
        "print(\"Vision model device:\", next(model.vision_model.parameters()).device)\n",
        "print(\"Vision projection layer device:\", next(model.vision_proj.parameters()).device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-5LmbTaJtny"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIGXaH1XjdzM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "980687f7-9773-4c6b-c8e4-f0b953974bf6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Printing the total number of parameters and the number of trainable parameters:\n",
            "Vision Encoder (SigLIP): {'Total': 92884224, 'Trainable': 0}\n",
            "Language Decoder (GPT-2): {'Total': 82783488, 'Trainable': 811008}\n",
            "Vision Projection Layer: {'Total': 590592, 'Trainable': 590592}\n",
            "Total CustomVLM: {'Total': 176258304, 'Trainable': 1401600}\n"
          ]
        }
      ],
      "source": [
        "# We count the number of parameters\n",
        "def count_parameters(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return {\"Total\": total, \"Trainable\": trainable}\n",
        "\n",
        "print(\"Printing the total number of parameters and the number of trainable parameters:\")\n",
        "print(\"Vision Encoder (SigLIP):\", count_parameters(model.vision_model))\n",
        "print(\"Language Decoder (GPT-2):\", count_parameters(model.language_model))\n",
        "print(\"Vision Projection Layer:\", count_parameters(model.vision_proj))\n",
        "print(\"Total CustomVLM:\", count_parameters(model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgM9u7Rmjho_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b6d672a-9759-492c-8579-fb380d2555ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating caption for sample 1\n",
            "Generated caption: 'T-S-S-S-S-S-S-S-S-S-S-S'\n",
            "\n",
            "Generating caption for sample 2\n",
            "Generated caption: 'T-S-S-S-S-S-S-S-S-S-S-S'\n",
            "\n",
            "Generating caption for sample 3\n",
            "Generated caption: 'm a big, big, big, big, big, big, big, big, big, big, big,'\n",
            "\n",
            "Generating caption for sample 4\n",
            "Generated caption: 'I-I-I-I-I-I-I-I-I-I-I-I'\n",
            "\n",
            "Generating caption for sample 5\n",
            "Generated caption: 'rst thing that I do is I go to the local, the local, the local, the local, the local, the'\n",
            "\n",
            "Generating caption for sample 6\n",
            "Generated caption: 'st of the two, the \"The Great and the Great\" is a \"The Great and the Great\" and the \"The'\n",
            "\n",
            "Generating caption for sample 7\n",
            "Generated caption: 'he other.\\n\\nThe first time I saw the video, I was in the middle of the night, and I was'\n",
            "\n",
            "Generating caption for sample 8\n",
            "Generated caption: 'T-S-S-S-S-S-S-S-S-S-S-S'\n",
            "\n",
            "Generating caption for sample 9\n",
            "Generated caption: 'st of the two, the \"The Great and the Great\" is a \"The Great and the Great\" and the \"The'\n",
            "\n",
            "Generating caption for sample 10\n",
            "Generated caption: 'st of the two, the \"S\" in the name of the \"S\" in the name of the \"S\" in'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Check whether the model can generate captions at all\n",
        "# Set model to eval mode\n",
        "model.eval()\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Generating caption for sample {i + 1}\")\n",
        "\n",
        "    # Load and preprocess image\n",
        "    image = ds[\"test\"][i][\"image\"]\n",
        "    pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"]\n",
        "    pixel_values = pixel_values.to(model.language_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vision encoding\n",
        "        vision_output = model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
        "        vision_embedding = vision_output.mean(dim=1)\n",
        "        vision_proj = model.vision_proj(vision_embedding).unsqueeze(1)  # [B, 1, D]\n",
        "\n",
        "        # Prepare prompt\n",
        "        prompt = \"caption en\"\n",
        "        tokenized = tokenizer(prompt, return_tensors=\"pt\", padding=False)\n",
        "        input_ids = tokenized[\"input_ids\"].to(model.language_model.device)\n",
        "        attention_mask = tokenized[\"attention_mask\"].to(model.language_model.device)\n",
        "\n",
        "        # Text embedding\n",
        "        input_embeds = model.language_model.transformer.wte(input_ids)\n",
        "        input_embeds = torch.cat([vision_proj, input_embeds], dim=1)\n",
        "\n",
        "        # Extend attention mask with prefix mask\n",
        "        prefix_mask = torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype).to(attention_mask.device)\n",
        "        attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "        # Generate text\n",
        "        generated_ids = model.language_model.generate(\n",
        "            inputs_embeds=input_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=30,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # Decode the caption\n",
        "        caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)[len(prompt):]\n",
        "        print(\"Generated caption:\", repr(caption))\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89tTN1e8xTjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63a4ed2c-4037-4bf4-ea89-18d2120793f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/10:   0%|          | 0/150 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n",
            "Epoch 1/10: 100%|██████████| 150/150 [04:42<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed. Average Train Loss: 3.2993\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 2.3526\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|██████████| 150/150 [04:42<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed. Average Train Loss: 1.4317\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.5089\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|██████████| 150/150 [04:42<00:00,  1.89s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed. Average Train Loss: 1.1175\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.38s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.2788\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|██████████| 150/150 [04:41<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed. Average Train Loss: 0.9998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 1.1197\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|██████████| 150/150 [04:42<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 completed. Average Train Loss: 0.9091\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.9938\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|██████████| 150/150 [04:41<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed. Average Train Loss: 0.8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.8784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|██████████| 150/150 [04:41<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 completed. Average Train Loss: 0.7864\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|██████████| 150/150 [04:42<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed. Average Train Loss: 0.7367\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.7038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|██████████| 150/150 [04:41<00:00,  1.87s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 completed. Average Train Loss: 0.7071\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.6536\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|██████████| 150/150 [04:41<00:00,  1.88s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 completed. Average Train Loss: 0.6684\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validating: 100%|██████████| 13/13 [00:17<00:00,  1.37s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Validation Loss: 0.6095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Prefix for all captions\n",
        "caption_prefix = \"caption en\"\n",
        "\n",
        "# Custom collate function\n",
        "def collate_fn(batch):\n",
        "    images = [image_processor(example[\"image\"], return_tensors=\"pt\")[\"pixel_values\"].squeeze(0) for example in batch]\n",
        "    captions = [caption_prefix + example[\"caption_3\"] for example in batch]\n",
        "    pixel_values = torch.stack(images)\n",
        "\n",
        "    tokenized = tokenizer(\n",
        "        captions,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": tokenized[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized[\"attention_mask\"],\n",
        "        \"labels\": tokenized[\"input_ids\"].clone()\n",
        "    }\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(ds[\"train\"], batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ds[\"val\"], batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Pad labels for prefix position with -100\n",
        "        prefix_ignore = torch.full((labels.size(0), 1), -100, dtype=labels.dtype, device=labels.device)\n",
        "        labels = torch.cat([prefix_ignore, labels], dim=1)\n",
        "\n",
        "        # Add prefix to attention mask\n",
        "        prefix_mask = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "        extended_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            image=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=extended_attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "        wandb.log({\n",
        "            \"train/loss\": loss.item(),\n",
        "            \"train/step\": epoch * len(train_loader) + step\n",
        "        })\n",
        "\n",
        "    avg_train_loss = total_loss / total_samples\n",
        "    print(f\"Epoch {epoch+1} completed. Average Train Loss: {avg_train_loss:.4f}\")\n",
        "    wandb.log({\"train/avg_epoch_loss\": avg_train_loss, \"epoch\": epoch + 1})\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Pad labels for prefix position with -100\n",
        "            prefix_ignore = torch.full((labels.size(0), 1), -100, dtype=labels.dtype, device=labels.device)\n",
        "            labels = torch.cat([prefix_ignore, labels], dim=1)\n",
        "\n",
        "            prefix_mask = torch.ones((attention_mask.size(0), 1), dtype=attention_mask.dtype, device=attention_mask.device)\n",
        "            extended_attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "            outputs = model(\n",
        "                image=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=extended_attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            val_loss += outputs.loss.item() * batch_size\n",
        "            val_samples += batch_size\n",
        "\n",
        "    avg_val_loss = val_loss / val_samples\n",
        "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "    wandb.log({\"val/loss\": avg_val_loss, \"epoch\": epoch + 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFK9pKxT-Jhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0e95275-a5cf-4f26-f5ff-62528c57564a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating captions: 100%|██████████| 3150/3150 [16:12<00:00,  3.24it/s]\n"
          ]
        }
      ],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "predictions = []\n",
        "\n",
        "# Loop through the full test dataset\n",
        "for i in tqdm(range(len(ds[\"test\"])), desc=\"Generating captions\"):\n",
        "    image = ds[\"test\"][i][\"image\"]\n",
        "    pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(model.language_model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vision encoding\n",
        "        vision_output = model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
        "        vision_embedding = vision_output.mean(dim=1)\n",
        "        vision_proj = model.vision_proj(vision_embedding).unsqueeze(1)  # [B, 1, D]\n",
        "\n",
        "        # Prepare prompt\n",
        "        prompt = \"caption en\"\n",
        "        tokenized = tokenizer(prompt, return_tensors=\"pt\", padding=False)\n",
        "        input_ids = tokenized[\"input_ids\"].to(model.language_model.device)\n",
        "        attention_mask = tokenized[\"attention_mask\"].to(model.language_model.device)\n",
        "\n",
        "        # Text embedding\n",
        "        input_embeds = model.language_model.transformer.wte(input_ids)\n",
        "        input_embeds = torch.cat([vision_proj, input_embeds], dim=1)\n",
        "\n",
        "        # Extend attention mask with prefix mask\n",
        "        prefix_mask = torch.ones((attention_mask.shape[0], 1), dtype=attention_mask.dtype).to(attention_mask.device)\n",
        "        attention_mask = torch.cat([prefix_mask, attention_mask], dim=1)\n",
        "\n",
        "        # Generate text\n",
        "        generated_ids = model.language_model.generate(\n",
        "            inputs_embeds=input_embeds,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=30,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # Decode caption (removing prompt part)\n",
        "        caption = tokenizer.decode(generated_ids[0], skip_special_tokens=True)[len(prompt):].strip()\n",
        "        predictions.append(caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ial5sVoWBNqT"
      },
      "outputs": [],
      "source": [
        "# Get the references\n",
        "# Define a variable to store the reference captions\n",
        "all_references = []\n",
        "for i in tqdm(range(len(ds[\"test\"])), desc=\"Collecting reference captions\"):\n",
        "    # Get the reference\n",
        "    reference_per_sample = []\n",
        "    for j in range(1,6):\n",
        "        reference = ds[\"test\"][i][f\"caption_{j}\"]\n",
        "        reference_per_sample.append(reference)\n",
        "        print(f\"The reference caption_{j}:\")\n",
        "        print(repr(reference))\n",
        "\n",
        "    print()\n",
        "    all_references.append(reference_per_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtqWQf7DEKl4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0149f1bd-54d8-4d24-d9d2-dd0c1ca6b1d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['A gray plane on the runway and the lawn beside .', 'A grey plane is on the runway by the lawn .', 'There is an airplane on the runway with a large lawn by the runway .', 'A plane is parked on the runway next to the grass .', 'There is a plane on the runway beside the grass .'], ['Three small planes parked in a line on the airport and a big plane behind them .', 'There are four aircraft on the open ground, The largest of which is three times as large as the smallest one .', 'There are many planes of different sizes in a clearing .', 'Four planes are parked on the runway .', 'Four planes of different sizes were on the marked ground .'], ['A plane parked in a line on the airport with some marks .', 'A white plane was parked on the instruction line .', 'An airplane parked in an open area with many containers next to it .', 'A plane is parked on the open space .', 'There is 1 plane on the ground marked .'], ['A small plane and a big plane parked next to boarding bridges .', 'A white plane and a gray plane parked at the boarding port  The white plane was about four times as large as the gray one .', 'Two planes of different sizes are neatly parked next to the buildings in the airport .', 'A large plane and a small plane are parked near the terminal .', 'Two planes are on the marked ground .'], ['Two planes parked next to boarding bridges .', 'Two aircraft were parked at the departure gates .', 'Two planes of different sizes are neatly parked next to the buildings in the airport .', 'Two planes are parked next to the terminal .', 'Two planes are on the marked ground .']]\n"
          ]
        }
      ],
      "source": [
        "# Check the format of the reference captions\n",
        "print(all_references[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hS827PUEhRN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1cf7ef3-832b-4669-d721-f754f8442575"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['is on the grass next to some trees .', 'is on the grass next to some buildings .', 'is on the grass next to some buildings .', 'many planes on the runway at the airport, There are many cars on the runway, And there are many buildings on the runway .', 'is on the grass next to some trees .']\n"
          ]
        }
      ],
      "source": [
        "# Check the format of the predicted captions. Each sample starts with a new line\n",
        "print(predictions[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6-ypijV2XMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ff83901-207c-44a6-af20-5912fbf224cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize references and predictions:\n",
        "tokenized_refs = [\n",
        "    [nltk.word_tokenize(ref.lower()) for ref in refs]\n",
        "    for refs in all_references\n",
        "]\n",
        "\n",
        "tokenized_hyps = [nltk.word_tokenize(pred.lower()) for pred in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HVsAM-UjEtN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f5ac00a-3838-4f41-b572-b3467e93df97"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['a',\n",
              "  'gray',\n",
              "  'plane',\n",
              "  'on',\n",
              "  'the',\n",
              "  'runway',\n",
              "  'and',\n",
              "  'the',\n",
              "  'lawn',\n",
              "  'beside',\n",
              "  '.'],\n",
              " ['a', 'grey', 'plane', 'is', 'on', 'the', 'runway', 'by', 'the', 'lawn', '.'],\n",
              " ['there',\n",
              "  'is',\n",
              "  'an',\n",
              "  'airplane',\n",
              "  'on',\n",
              "  'the',\n",
              "  'runway',\n",
              "  'with',\n",
              "  'a',\n",
              "  'large',\n",
              "  'lawn',\n",
              "  'by',\n",
              "  'the',\n",
              "  'runway',\n",
              "  '.'],\n",
              " ['a',\n",
              "  'plane',\n",
              "  'is',\n",
              "  'parked',\n",
              "  'on',\n",
              "  'the',\n",
              "  'runway',\n",
              "  'next',\n",
              "  'to',\n",
              "  'the',\n",
              "  'grass',\n",
              "  '.'],\n",
              " ['there',\n",
              "  'is',\n",
              "  'a',\n",
              "  'plane',\n",
              "  'on',\n",
              "  'the',\n",
              "  'runway',\n",
              "  'beside',\n",
              "  'the',\n",
              "  'grass',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "tokenized_refs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9BGh1zclFI-"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-2\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/2, 1/2),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-2: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puRb6hgwEBAs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c10ad477-a6d7-4ddd-f734-2a83c3046d6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus BLEU-2: 46.94\n"
          ]
        }
      ],
      "source": [
        "# Corpus-level BLEU-2\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/2, 1/2),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-2: {corpus_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOQ1uwXglOJF"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-3\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/3, 1/3, 1/3),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-3: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S23MU9ALECXX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8efd4b02-c5ed-4694-c4aa-0c31f843b17e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus BLEU-3: 35.25\n"
          ]
        }
      ],
      "source": [
        "# Corpus-level BLEU-3\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/3, 1/3, 1/3),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-3: {corpus_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52GwUtcJk_ki"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-4\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/4, 1/4, 1/4, 1/4),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-4: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus-level BLEU-4\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/4, 1/4, 1/4, 1/4),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-4: {corpus_score*100:.2f}\")"
      ],
      "metadata": {
        "id": "b4KU-DsACcux",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72196722-01f5-4bb5-fda3-f96b027c55fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Corpus BLEU-4: 28.16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFe4Ho28BBA-"
      },
      "outputs": [],
      "source": [
        "# Go on to calculate ROUGE scores\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndmk9F4VaR9z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure tokenizer\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def rouge_n(ref: str, hyp: str, n: int = 4):\n",
        "    ref_toks = nltk.word_tokenize(ref.lower())\n",
        "    hyp_toks = nltk.word_tokenize(hyp.lower())\n",
        "    ref_ngrams = list(nltk.ngrams(ref_toks, n))\n",
        "    hyp_ngrams = list(nltk.ngrams(hyp_toks, n))\n",
        "    ref_counts = Counter(ref_ngrams)\n",
        "    hyp_counts = Counter(hyp_ngrams)\n",
        "    overlap = sum(min(ref_counts[ng], hyp_counts[ng]) for ng in ref_counts)\n",
        "    recall = overlap / max(len(ref_ngrams), 1)\n",
        "    precision = overlap / max(len(hyp_ngrams), 1)\n",
        "    f1 = 2 * recall * precision / (recall + precision + 1e-8)\n",
        "    return (recall, precision, f1)\n",
        "\n",
        "# Compute ROUGE-2\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=2)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-2 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-2 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-2 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha0vcqTFYx79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24909d11-c635-4d75-fd0c-67db148509e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AVERAGE ROUGE-2 METRICS ===\n",
            "Recall:    23.28\n",
            "Precision: 28.88\n",
            "F1:        25.38\n"
          ]
        }
      ],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-2 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3iJDsYWmLq3"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE-3\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=3)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-3 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-3 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-3 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz9cBH2KY1mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0a1da5a-00f8-450c-bdb5-00087a2504dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AVERAGE ROUGE-3 METRICS ===\n",
            "Recall:    15.11\n",
            "Precision: 19.06\n",
            "F1:        16.62\n"
          ]
        }
      ],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-3 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GbFHU6QnYuJ"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE-4\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=4)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-4 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-4 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-4 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MlAOzLHs9je",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85a47db0-939e-4231-a191-4a5bf1b9e202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== AVERAGE ROUGE-4 METRICS ===\n",
            "Recall:    10.74\n",
            "Precision: 13.93\n",
            "F1:        11.96\n"
          ]
        }
      ],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-4 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4hhPU2Z93U"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/DI725 - Transformers and Attention-based Deep Networks/Term Project/siglip-gpt2-custom_vlm_finetuned\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Save merged language model (now a clean GPT-2)\n",
        "model.language_model.save_pretrained(save_dir)\n",
        "\n",
        "# Save vision encoder and image processor\n",
        "model.vision_model.save_pretrained(f\"{save_dir}/vision_encoder\")\n",
        "image_processor.save_pretrained(f\"{save_dir}/vision_encoder\")\n",
        "\n",
        "# Save vision projection layer\n",
        "torch.save(model.vision_proj.state_dict(), f\"{save_dir}/vision_proj.pt\")\n",
        "\n",
        "# Save config for reinitialization\n",
        "import json\n",
        "config = {\n",
        "    \"vision_encoder_path\": \"vision_encoder\",\n",
        "    \"language_model_path\": \".\",\n",
        "    \"vision_proj_path\": \"vision_proj.pt\",\n",
        "    \"vision_hidden_size\": model.vision_proj.in_features,\n",
        "    \"language_hidden_size\": model.vision_proj.out_features\n",
        "}\n",
        "with open(os.path.join(save_dir, \"custom_vlm_config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DONE\")"
      ],
      "metadata": {
        "id": "eBey5YrPGnY-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8be44776-0e95-4f1c-d0d0-33ee07f2a487"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DONE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AvaU2cHRC4jE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNxbGw6eyHIq3hSqa6lRjOr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}