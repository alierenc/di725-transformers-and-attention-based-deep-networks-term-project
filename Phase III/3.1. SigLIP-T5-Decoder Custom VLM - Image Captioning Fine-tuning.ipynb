{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alierenc/di725-transformers-and-attention-based-deep-networks-term-project/blob/main/Phase%20III/3.1.%20SigLIP-T5-Decoder%20Custom%20VLM%20-%20Image%20Captioning%20Fine-tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxRuDcZeWlmA"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "hf_token = \" \" # Huggingface token\n",
        "login(token = hf_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mSzW00Eq6ykL"
      },
      "outputs": [],
      "source": [
        "# Import and log in wandb\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n",
        "# Initialize W&B run\n",
        "wandb.init(project=\"term-project-vision-language-model\", name=\"siglip-t5decoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieFLxaGQyKmq"
      },
      "outputs": [],
      "source": [
        "!pip install -U datasets\n",
        "!pip install bitsandbytes --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1KNOSns1Okr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# Load the dataset of full riscm\n",
        "ds = load_dataset('caglarmert/full_riscm')\n",
        "\n",
        "full = ds[\"train\"]\n",
        "\n",
        "# test   = indices [0, 3150)\n",
        "test_ds = full.select(range(3150))\n",
        "\n",
        "# validation = indices [3150, 6300)\n",
        "val_ds = full.select(range(3150, 6300))\n",
        "\n",
        "# train  = indices [6300, end)\n",
        "train_ds = full.select(range(6300, len(full)))\n",
        "\n",
        "# bundle into a DatasetDict\n",
        "ds = DatasetDict({\n",
        "    \"test\": test_ds,\n",
        "    \"val\": val_ds,\n",
        "    \"train\": train_ds,\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8ciL7lUpvAV"
      },
      "outputs": [],
      "source": [
        "ds[\"test\"][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "class CustomVLM(nn.Module):\n",
        "    def __init__(self, vision_model, language_model, vision_hidden_size, language_hidden_size):\n",
        "        super().__init__()\n",
        "        self.vision_model = vision_model\n",
        "        self.language_model = language_model\n",
        "        self.vision_proj = nn.Linear(vision_hidden_size, language_hidden_size)\n",
        "\n",
        "    def forward(self, image, input_ids=None, attention_mask=None, labels=None):\n",
        "        # Step 1: Encode image\n",
        "        vision_output = self.vision_model(pixel_values=image).last_hidden_state  # [B, N, D]\n",
        "        vision_embedding = vision_output.mean(dim=1)                             # [B, D]\n",
        "        encoder_hidden_states = self.vision_proj(vision_embedding).unsqueeze(1)  # [B, 1, d_model]\n",
        "\n",
        "        # Step 2: Package encoder output for T5\n",
        "        encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_states)\n",
        "\n",
        "        # Step 3: Decode using prompt + image context\n",
        "        output = self.language_model(\n",
        "            input_ids=input_ids,                # prompt like \"caption en\"\n",
        "            attention_mask=attention_mask,      # attention mask for prompt\n",
        "            encoder_outputs=encoder_outputs,    # SigLIP embedding as context\n",
        "            labels=labels                       # optional: ground-truth caption for training\n",
        "        )\n",
        "\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "ipVYrBherD78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdLaepmWSWD3"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    SiglipVisionModel,\n",
        "    AutoImageProcessor,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "import bitsandbytes\n",
        "import torch\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# QLoRA quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Load full T5 model (needed for decoder + lm_head)\n",
        "language_model = T5ForConditionalGeneration.from_pretrained(\n",
        "    \"t5-base\",\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Prepare for QLoRA\n",
        "language_model = prepare_model_for_kbit_training(language_model)\n",
        "\n",
        "# Define LoRA config (on full model â€” affects both encoder & decoder if needed)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],  # extend to 'k', 'o', etc. for more aggressive tuning\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "language_model = get_peft_model(language_model, lora_config)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load SigLIP vision encoder\n",
        "vision_model = SiglipVisionModel.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "vision_model.requires_grad_(False)\n",
        "\n",
        "# Load image processor\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"google/siglip-base-patch16-224\")\n",
        "\n",
        "# Determine hidden sizes\n",
        "vision_hidden_size = vision_model.config.hidden_size     # e.g. 768\n",
        "language_hidden_size = language_model.config.d_model     # 768 for t5-base\n",
        "\n",
        "# Initialize CustomVLM with full language model\n",
        "model = CustomVLM(\n",
        "    vision_model=vision_model,\n",
        "    language_model=language_model,\n",
        "    vision_hidden_size=vision_hidden_size,\n",
        "    language_hidden_size=language_hidden_size\n",
        ")\n",
        "\n",
        "# Move to device\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BwqZf20PC0JI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "# We check whether the model can produce captions\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "max_new_tokens = 30\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Generating caption for sample {i + 1}\")\n",
        "\n",
        "    # Preprocess image\n",
        "    image = ds[\"test\"][i][\"image\"]\n",
        "    pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vision encoding inside VLM forward\n",
        "        vision_output = model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
        "        vision_embedding = vision_output.mean(dim=1)\n",
        "        encoder_hidden_states = model.vision_proj(vision_embedding).unsqueeze(1)\n",
        "\n",
        "        # Wrap as BaseModelOutput\n",
        "        encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_states)\n",
        "\n",
        "        # Prompt setup\n",
        "        prompt = \"caption en\"\n",
        "        tokenized = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        input_ids = tokenized[\"input_ids\"]\n",
        "        attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "        # Remove <pad> if it appears as the first token\n",
        "        if input_ids[0, 0] == tokenizer.pad_token_id:\n",
        "            input_ids = input_ids[:, 1:]\n",
        "            attention_mask = attention_mask[:, 1:]\n",
        "\n",
        "        decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        caption = decoded.replace(tokenizer.pad_token, \"\").replace(prompt.strip(), \"\").strip()\n",
        "\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.language_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # Decode output\n",
        "        decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        caption = decoded.replace(prompt, \"\").strip()\n",
        "        caption = decoded.replace(\"<pad>\", \"\").strip()\n",
        "        print(\"Generated caption:\", repr(caption))\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "S9lTIhyzqsiz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmyL4wRymzjg"
      },
      "outputs": [],
      "source": [
        "kprint(\"Language model device:\", next(model.language_model.parameters()).device)\n",
        "print(\"Vision model device:\", next(model.vision_model.parameters()).device)\n",
        "print(\"Vision projection layer device:\", next(model.vision_proj.parameters()).device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-5LmbTaJtny"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aIGXaH1XjdzM"
      },
      "outputs": [],
      "source": [
        "def count_parameters(module):\n",
        "    total = sum(p.numel() for p in module.parameters())\n",
        "    trainable = sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
        "    return {\"Total\": total, \"Trainable\": trainable}\n",
        "\n",
        "print(\"Printing the total number of parameters and the number of trainable parameters:\")\n",
        "print(\"Vision Encoder (SigLIP):\", count_parameters(model.vision_model))\n",
        "print(\"Vision Projection Layer:\", count_parameters(model.vision_proj))\n",
        "print(\"Language Model (full T5):\", count_parameters(model.language_model))\n",
        "\n",
        "# Count only T5 decoder (within the full model)\n",
        "print(\"T5 Decoder (only):\", count_parameters(model.language_model.base_model.decoder))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89tTN1e8xTjl"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Collate function for T5 captioning\n",
        "def collate_fn(batch):\n",
        "    images = [image_processor(example[\"image\"], return_tensors=\"pt\")[\"pixel_values\"].squeeze(0) for example in batch]\n",
        "    captions = [example[\"caption_3\"] for example in batch]\n",
        "\n",
        "    pixel_values = torch.stack(images)\n",
        "\n",
        "    # For T5: prompt goes into input_ids, caption goes into labels\n",
        "    prompts = [\"caption en\"] * len(captions)\n",
        "    tokenized_input = tokenizer(prompts, padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "    tokenized_labels = tokenizer(captions, padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
        "\n",
        "    return {\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"input_ids\": tokenized_input[\"input_ids\"],\n",
        "        \"attention_mask\": tokenized_input[\"attention_mask\"],\n",
        "        \"labels\": tokenized_labels[\"input_ids\"]  # T5 will shift internally\n",
        "    }\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(ds[\"train\"], batch_size=256, shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(ds[\"val\"], batch_size=256, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "\n",
        "    for step, batch in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")):\n",
        "        pixel_values = batch[\"pixel_values\"].to(device)\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = batch[\"labels\"].to(device)\n",
        "\n",
        "        # Replace <pad> tokens in labels with -100 so they are ignored in loss\n",
        "        labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            image=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        batch_size = input_ids.size(0)\n",
        "        total_loss += loss.item() * batch_size\n",
        "        total_samples += batch_size\n",
        "\n",
        "        # wandb.log({\n",
        "        #     \"train/loss\": loss.item(),\n",
        "        #     \"train/step\": epoch * len(train_loader) + step\n",
        "        # })\n",
        "\n",
        "\n",
        "    avg_train_loss = total_loss / total_samples\n",
        "    print(f\"Epoch {epoch+1} completed. Average Train Loss: {avg_train_loss:.4f}\")\n",
        "    # wandb.log({\"train/avg_epoch_loss\": avg_train_loss, \"epoch\": epoch + 1})\n",
        "\n",
        "    # Validation Loop\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
        "            pixel_values = batch[\"pixel_values\"].to(device)\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "\n",
        "            # Same label cleaning\n",
        "            labels[labels == tokenizer.pad_token_id] = -100\n",
        "\n",
        "            outputs = model(\n",
        "                image=pixel_values,\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            batch_size = input_ids.size(0)\n",
        "            val_loss += outputs.loss.item() * batch_size\n",
        "            val_samples += batch_size\n",
        "\n",
        "    avg_val_loss = val_loss / val_samples\n",
        "    print(f\"Average Validation Loss: {avg_val_loss:.4f}\")\n",
        "    # wandb.log({\"val/loss\": avg_val_loss, \"epoch\": epoch + 1})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFK9pKxT-Jhv"
      },
      "outputs": [],
      "source": [
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "max_new_tokens = 30\n",
        "eos_token_id = tokenizer.eos_token_id\n",
        "predictions = []\n",
        "\n",
        "for i in range(10):\n",
        "    print(f\"Generating caption for sample {i + 1}\")\n",
        "\n",
        "    # Preprocess image\n",
        "    image = ds[\"test\"][i][\"image\"]\n",
        "    pixel_values = image_processor(image, return_tensors=\"pt\")[\"pixel_values\"].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Vision encoding inside VLM forward\n",
        "        vision_output = model.vision_model(pixel_values=pixel_values).last_hidden_state\n",
        "        vision_embedding = vision_output.mean(dim=1)\n",
        "        encoder_hidden_states = model.vision_proj(vision_embedding).unsqueeze(1)\n",
        "\n",
        "        # Wrap as BaseModelOutput\n",
        "        encoder_outputs = BaseModelOutput(last_hidden_state=encoder_hidden_states)\n",
        "\n",
        "        # Prompt setup\n",
        "        prompt = \"caption en\"\n",
        "        tokenized = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "        input_ids = tokenized[\"input_ids\"]\n",
        "        attention_mask = tokenized[\"attention_mask\"]\n",
        "\n",
        "        # Remove <pad> if it appears as the first token\n",
        "        if input_ids[0, 0] == tokenizer.pad_token_id:\n",
        "            input_ids = input_ids[:, 1:]\n",
        "            attention_mask = attention_mask[:, 1:]\n",
        "\n",
        "        decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        caption = decoded.replace(tokenizer.pad_token, \"\").replace(prompt.strip(), \"\").strip()\n",
        "\n",
        "\n",
        "        # Generate\n",
        "        generated_ids = model.language_model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            eos_token_id=eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            do_sample=False\n",
        "        )\n",
        "\n",
        "        # Decode output\n",
        "        decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
        "        caption = decoded.replace(prompt, \"\").strip()\n",
        "        caption = decoded.replace(\"<pad>\", \"\").strip()\n",
        "        predictions.append(caption)\n",
        "        print(\"Generated caption:\", repr(caption))\n",
        "        print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ial5sVoWBNqT"
      },
      "outputs": [],
      "source": [
        "# Get the references\n",
        "# Define a varible to store the reference captions\n",
        "all_references = []\n",
        "for i in range(len(ds[\"test\"])):\n",
        "    # Get the reference\n",
        "    reference_per_sample = []\n",
        "    for j in range(1,6):\n",
        "        reference = ds[\"test\"][i][f\"caption_{j}\"]\n",
        "        reference_per_sample.append(reference)\n",
        "        print(f\"The reference caption_{j}:\")\n",
        "        print(repr(reference))\n",
        "\n",
        "    print()\n",
        "    all_references.append(reference_per_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtqWQf7DEKl4"
      },
      "outputs": [],
      "source": [
        "# Check the format of the reference captions\n",
        "print(all_references[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hS827PUEhRN"
      },
      "outputs": [],
      "source": [
        "# Check the format of the predicted captions. Each sample starts with a new line\n",
        "print(predictions[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6-ypijV2XMP"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu, SmoothingFunction\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Tokenize references and predictions:\n",
        "tokenized_refs = [\n",
        "    [nltk.word_tokenize(ref.lower()) for ref in refs]\n",
        "    for refs in all_references\n",
        "]\n",
        "\n",
        "tokenized_hyps = [nltk.word_tokenize(pred.lower()) for pred in predictions]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_HVsAM-UjEtN"
      },
      "outputs": [],
      "source": [
        "tokenized_refs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9BGh1zclFI-"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-2\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/2, 1/2),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-2: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus-level BLEU-2\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/2, 1/2),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-2: {corpus_score*100:.2f}\")"
      ],
      "metadata": {
        "id": "0LFVQTlOIWJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOQ1uwXglOJF"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-3\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/3, 1/3, 1/3),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-3: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus-level BLEU-3\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/3, 1/3, 1/3),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-3: {corpus_score*100:.2f}\")"
      ],
      "metadata": {
        "id": "7DRXEFMkIYqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52GwUtcJk_ki"
      },
      "outputs": [],
      "source": [
        "# Sentence-level BLEU-4\n",
        "smooth = SmoothingFunction().method1\n",
        "for i, (refs_per_sample, hyp_tok) in enumerate(zip(tokenized_refs, tokenized_hyps)):\n",
        "    scores = []\n",
        "    for refs_tok in refs_per_sample:\n",
        "        score = sentence_bleu(\n",
        "            [refs_tok],\n",
        "            hyp_tok,\n",
        "            weights=(1/4, 1/4, 1/4, 1/4),\n",
        "            smoothing_function=smooth\n",
        "        )\n",
        "        scores.append(score)\n",
        "    max_score = max(scores)\n",
        "    print(f\"Example {i+1:2d} BLEU-4: {max_score*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Corpus-level BLEU-4\n",
        "# corpus_bleu expects list-of-list-of-tokens refs, and list-of-tokens hyps\n",
        "corpus_score = corpus_bleu(\n",
        "    tokenized_refs,\n",
        "    tokenized_hyps,\n",
        "    weights=(1/4, 1/4, 1/4, 1/4),\n",
        "    smoothing_function=smooth\n",
        ")\n",
        "print(f\"\\nCorpus BLEU-4: {corpus_score*100:.2f}\")"
      ],
      "metadata": {
        "id": "mM7yA4qEIbG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFe4Ho28BBA-"
      },
      "outputs": [],
      "source": [
        "# Go on to calculate ROUGE scores\n",
        "!pip install rouge-score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndmk9F4VaR9z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "\n",
        "# Ensure tokenizer\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "def rouge_n(ref: str, hyp: str, n: int = 4):\n",
        "    ref_toks = nltk.word_tokenize(ref.lower())\n",
        "    hyp_toks = nltk.word_tokenize(hyp.lower())\n",
        "    ref_ngrams = list(nltk.ngrams(ref_toks, n))\n",
        "    hyp_ngrams = list(nltk.ngrams(hyp_toks, n))\n",
        "    ref_counts = Counter(ref_ngrams)\n",
        "    hyp_counts = Counter(hyp_ngrams)\n",
        "    overlap = sum(min(ref_counts[ng], hyp_counts[ng]) for ng in ref_counts)\n",
        "    recall = overlap / max(len(ref_ngrams), 1)\n",
        "    precision = overlap / max(len(hyp_ngrams), 1)\n",
        "    f1 = 2 * recall * precision / (recall + precision + 1e-8)\n",
        "    return (recall, precision, f1)\n",
        "\n",
        "# Compute ROUGE-2\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=2)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-2 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-2 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-2 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ha0vcqTFYx79"
      },
      "outputs": [],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-2 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3iJDsYWmLq3"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE-3\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=3)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-3 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-3 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-3 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz9cBH2KY1mv"
      },
      "outputs": [],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-3 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-GbFHU6QnYuJ"
      },
      "outputs": [],
      "source": [
        "# Compute ROUGE-4\n",
        "all_recalls, all_precisions, all_f1s = [], [], []\n",
        "for refs, pred in zip(all_references, predictions):\n",
        "    recalls_per_sample, precisions_per_sample, f1s_per_sample = [], [], []\n",
        "    for ref in refs:\n",
        "        r, p, f = rouge_n(ref, pred, n=4)\n",
        "        recalls_per_sample.append(r)\n",
        "        precisions_per_sample.append(p)\n",
        "        f1s_per_sample.append(f)\n",
        "\n",
        "    max_score = max(f1s_per_sample)\n",
        "    max_index = f1s_per_sample.index(max_score)\n",
        "    all_recalls.append(recalls_per_sample[max_index])\n",
        "    all_precisions.append(precisions_per_sample[max_index])\n",
        "    all_f1s.append(f1s_per_sample[max_index])\n",
        "    print(f\"REF:  {refs[max_index]!r}\")\n",
        "    print(f\"HYP:  {pred!r}\")\n",
        "    print(f\"   ROUGE-4 Recall:    {recalls_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-4 Precision: {precisions_per_sample[max_index] * 100:.2f}%\")\n",
        "    print(f\"   ROUGE-4 F1:        {f1s_per_sample[max_index] * 100:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5MlAOzLHs9je"
      },
      "outputs": [],
      "source": [
        "# Report overall averages\n",
        "avg_r = sum(all_recalls) / len(all_recalls)\n",
        "avg_p = sum(all_precisions) / len(all_precisions)\n",
        "avg_f = sum(all_f1s) / len(all_f1s)\n",
        "print(\"=== AVERAGE ROUGE-4 METRICS ===\")\n",
        "print(f\"Recall:    {avg_r*100:.2f}\")\n",
        "print(f\"Precision: {avg_p*100:.2f}\")\n",
        "print(f\"F1:        {avg_f*100:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "save_dir = \"/content/drive/MyDrive/DI725 - Transformers and Attention-based Deep Networks/Term Project/siglip-t5-custom_vlm_finetuned\"\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Save tokenizer\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "\n",
        "# Save merged language model (now a clean T5)\n",
        "model.language_model.save_pretrained(save_dir)\n",
        "\n",
        "# Save vision encoder and image processor\n",
        "model.vision_model.save_pretrained(f\"{save_dir}/vision_encoder\")\n",
        "image_processor.save_pretrained(f\"{save_dir}/vision_encoder\")\n",
        "\n",
        "# Save vision projection layer\n",
        "torch.save(model.vision_proj.state_dict(), f\"{save_dir}/vision_proj.pt\")\n",
        "\n",
        "# Save config for reinitialization\n",
        "import json\n",
        "config = {\n",
        "    \"vision_encoder_path\": \"vision_encoder\",\n",
        "    \"language_model_path\": \".\",\n",
        "    \"vision_proj_path\": \"vision_proj.pt\",\n",
        "    \"vision_hidden_size\": model.vision_proj.in_features,\n",
        "    \"language_hidden_size\": model.vision_proj.out_features\n",
        "}\n",
        "with open(os.path.join(save_dir, \"custom_vlm_config.json\"), \"w\") as f:\n",
        "    json.dump(config, f, indent=2)\n"
      ],
      "metadata": {
        "id": "u_Dq8sYbJLon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4hhPU2Z93U"
      },
      "outputs": [],
      "source": [
        "print(\"DONE\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPIz4ZcL1GnUpR1lTh8zjFD",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}